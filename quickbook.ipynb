{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ratulb/llmlite.mojo/blob/main/quickbook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Mojo quickbook**"
      ],
      "metadata": {
        "id": "Gh82E_KBDg3z"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "buOgxm25ONit"
      },
      "outputs": [],
      "source": [
        "!curl -ssL https://magic.modular.com/ | bash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FVZvyhRiONiw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['PATH'] +=':/root/.modular/bin'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqFD0EK0ONiw"
      },
      "outputs": [],
      "source": [
        "!magic init llmlite.mojo --format mojoproject\n",
        "%cd llmlite.mojo/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile common_utils.mojo\n",
        "\n",
        "from python import Python, PythonObject\n",
        "from tensors import Tensor\n",
        "from memory import UnsafePointer, memcpy\n",
        "\n",
        "\n",
        "fn int_varia_list_to_str(list: VariadicList[Int]) -> String:\n",
        "    s = String(\"[\")\n",
        "    for idx in range(len(list)):\n",
        "        s += list[idx].__str__()\n",
        "        if idx != len(list) - 1:\n",
        "            s += \", \"\n",
        "    s += \"]\"\n",
        "    return s\n",
        "\n",
        "\n",
        "fn numpy_dtype(dtype: DType) raises -> PythonObject:\n",
        "    np = Python.import_module(\"numpy\")\n",
        "    if dtype == DType.float32:\n",
        "        return np.float32\n",
        "    elif dtype == DType.float64:\n",
        "        return np.float64\n",
        "    elif dtype == DType.int8:\n",
        "        return np.int8\n",
        "    elif dtype == DType.uint8:\n",
        "        return np.uint8\n",
        "    elif dtype == DType.bool:\n",
        "        return np.bool\n",
        "    elif dtype == DType.uint16:\n",
        "        return np.uint16\n",
        "    elif dtype == DType.uint64:\n",
        "        return np.uint64\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "\n",
        "fn ndarray_ptr[\n",
        "    dtype: DType\n",
        "](ndarray: PythonObject) raises -> UnsafePointer[Scalar[dtype]]:\n",
        "    return ndarray.__array_interface__[\"data\"][0].unsafe_get_as_pointer[dtype]()\n",
        "\n",
        "\n",
        "fn to_ndarray[\n",
        "    axes_sizes: Int, dtype: DType, //\n",
        "](tensor: Tensor[axes_sizes, dtype]) raises -> PythonObject:\n",
        "    np = Python.import_module(\"numpy\")\n",
        "    ndarray = np.zeros(tensor.numels(), dtype=numpy_dtype(tensor.datatype))\n",
        "    ndarray_ptr = ndarray_ptr[dtype](ndarray)\n",
        "    buffer_ptr = tensor.unsafe_ptr()\n",
        "    memcpy(ndarray_ptr, buffer_ptr, tensor.numels())\n",
        "    return ndarray"
      ],
      "metadata": {
        "id": "5hRf0ytlYTxQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaxB1auxONix"
      },
      "outputs": [],
      "source": [
        "%%writefile tensors.mojo\n",
        "\n",
        "### Mojo Tensor\n",
        "### Implement tensor library in mojo from first principles\n",
        "\n",
        "from math import iota, exp\n",
        "from random import randn, seed\n",
        "from utils import StaticTuple, Variant\n",
        "from time import perf_counter_ns\n",
        "from algorithm import vectorize\n",
        "from sys import simdwidthof\n",
        "from memory import UnsafePointer, Pointer, memcpy, memset, memset_zero\n",
        "from common_utils import int_varia_list_to_str\n",
        "\n",
        "#alias Tensors = Variant[Tensor, Tensor[_, _]]\n",
        "\n",
        "struct Tensor[axes_sizes: Int = 1, dtype: DType = DType.float32](\n",
        "    Copyable & Movable & Sized & Stringable\n",
        "):\n",
        "    # Gradients are float32\n",
        "    #alias TensorAlias = UnsafePointer[Self.UnboundTensor, origin=MutableOrigin.empty]\n",
        "    #alias TensorAlias = UnsafePointer[Self.UnboundTensor, origin=MutableAnyOrigin]\n",
        "    alias TensorAlias = UnsafePointer[Tensor[axes_sizes=*_, dtype=DType.float32]]\n",
        "    var shape: Shape[axes_sizes]\n",
        "    var data: UnsafePointer[Scalar[dtype]]\n",
        "    var datatype: DType\n",
        "    var requires_grad: Bool\n",
        "    var grad: Self.TensorAlias\n",
        "    var op: Optional[StaticString]\n",
        "    var parents: List[Self.TensorAlias]\n",
        "    var _backward: Optional[fn () escaping raises -> None]\n",
        "\n",
        "    fn __init__(\n",
        "        out self, *tensor_shapes: Int, requires_grad: Bool = False\n",
        "    ) raises:\n",
        "        axes_dims = Self.init_shape[axes_sizes](tensor_shapes)\n",
        "        self = Self(axes_dims)\n",
        "\n",
        "    @staticmethod\n",
        "    fn init_shape[\n",
        "        axes_count: Int\n",
        "    ](axes_spans: VariadicList[Int],) raises -> StaticTuple[Int, axes_count]:\n",
        "        axes_dims = StaticTuple[Int, axes_count](-1)\n",
        "        if len(axes_spans) != axes_count:\n",
        "            err = (\n",
        "                \"Tensor dimension = \"\n",
        "                + String(axes_count)\n",
        "                + \" and args count = \"\n",
        "                + String(len(axes_spans))\n",
        "                + \" mismatch\"\n",
        "            )\n",
        "            raise Error(err)\n",
        "        for i in range(axes_count):\n",
        "            axes_dims[i] = axes_spans[i]\n",
        "        return axes_dims\n",
        "\n",
        "    fn __init__(\n",
        "        out self,\n",
        "        axes_dims: StaticTuple[Int, axes_sizes],\n",
        "        requires_grad: Bool = False,\n",
        "    ):\n",
        "        self.shape = Shape[axes_sizes](axes_dims)\n",
        "        self.datatype = dtype\n",
        "        self.data = UnsafePointer[Scalar[dtype]].alloc(self.shape.numels)\n",
        "        self.requires_grad = requires_grad\n",
        "        self.grad = Self.TensorAlias()\n",
        "        self.op = None\n",
        "        self.parents = List[Self.TensorAlias](capacity=0)\n",
        "        self._backward = None\n",
        "\n",
        "    fn __getitem__(self, indices: List[Int]) raises -> Scalar[dtype]:\n",
        "        static_tuple = StaticTuple[Int, axes_sizes](0)\n",
        "        for i in range(axes_sizes):\n",
        "            static_tuple[i] = indices[i]\n",
        "        index = self.shape.flatten_index(static_tuple)\n",
        "        if index == -1:\n",
        "            raise Error(\"__getitem__(indices): Invalid indices\")\n",
        "        return (self.data + index)[]\n",
        "\n",
        "    fn __getitem__(self, *indices: Int) raises -> Scalar[dtype]:\n",
        "        index = self.shape.flatten_index(indices)\n",
        "        if index == -1:\n",
        "            raise Error(\"__getitem__(*indices): Invalid indices\")\n",
        "        return (self.data + index)[]\n",
        "\n",
        "    fn __setitem__(self, *indices: Int, value: Scalar[dtype]) raises:\n",
        "        index = self.shape.flatten_index(indices)\n",
        "        if index == -1:\n",
        "            raise Error(\"__setitem__(*indices): Invalid indices\")\n",
        "        (self.data + index)[] = value\n",
        "\n",
        "    fn __moveinit__(out self, owned other: Self):\n",
        "        self.data = UnsafePointer[Scalar[dtype]].alloc(other.numels())\n",
        "        memcpy(self.data, other.data, other.numels())\n",
        "        self.shape = other.shape^\n",
        "        self.datatype = other.datatype\n",
        "        self.requires_grad = other.requires_grad\n",
        "        self.grad = Self.TensorAlias()  # Not moving grad\n",
        "        self.op = other.op\n",
        "        self.parents = List[Self.TensorAlias](capacity=0)  # Not moving parents\n",
        "        self._backward = other._backward\n",
        "\n",
        "    fn __copyinit__(out self, other: Self):\n",
        "        self.shape = other.shape\n",
        "        self.data = UnsafePointer[Scalar[dtype]].alloc(other.numels())\n",
        "        memcpy(self.data, other.data, other.numels())\n",
        "        self.datatype = other.datatype\n",
        "        self.requires_grad = other.requires_grad\n",
        "        self.grad = Self.TensorAlias()  # Not copying grad\n",
        "        self.op = other.op\n",
        "        self.parents = List[Self.TensorAlias](capacity=0)  # Not copying parents\n",
        "        self._backward = other._backward\n",
        "\n",
        "    fn __del__(owned self):\n",
        "        self.data.free()\n",
        "\n",
        "    fn __len__(self) -> Int:\n",
        "        return self.numels()\n",
        "\n",
        "    @always_inline\n",
        "    fn numels(self) -> Int:\n",
        "        return self.shape.numels\n",
        "\n",
        "    @always_inline\n",
        "    fn ndim(self) -> Int:\n",
        "        return self.shape.ndim\n",
        "\n",
        "    fn all_true(self: Tensor[dtype = DType.bool]) -> Bool:\n",
        "        fn all_truthy(ambivalent: Scalar[DType.bool]) -> Bool:\n",
        "            return ambivalent == True\n",
        "\n",
        "        return self.for_all(all_truthy)\n",
        "\n",
        "    fn any_true(self: Tensor[dtype = DType.bool]) -> Bool:\n",
        "        fn any_truthy(ambivalent: Scalar[DType.bool]) -> Bool:\n",
        "            return ambivalent == True\n",
        "\n",
        "        return self.any(any_truthy)\n",
        "\n",
        "    fn for_all[\n",
        "        simd_width: Int = simdwidthof[dtype]()\n",
        "    ](self, pred: fn (Scalar[dtype]) -> Bool) -> Bool:\n",
        "        num_elems = self.numels()\n",
        "        simd_blocks = num_elems // simd_width\n",
        "        remaining = num_elems % simd_width\n",
        "\n",
        "        for i in range(simd_blocks):\n",
        "            vector = self.data.load[width=simd_width](i * simd_width)\n",
        "            for j in range(simd_width):\n",
        "                if not pred(vector[j]):\n",
        "                    return False\n",
        "        for k in range(remaining):\n",
        "            if not pred(self.data.load[width=1](simd_blocks * simd_width + k)):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    fn any[\n",
        "        simd_width: Int = simdwidthof[dtype]()\n",
        "    ](self, pred: fn (Scalar[dtype]) -> Bool) -> Bool:\n",
        "        num_elems = self.numels()\n",
        "        simd_blocks = num_elems // simd_width\n",
        "        remaining = num_elems % simd_width\n",
        "\n",
        "        for i in range(simd_blocks):\n",
        "            vector = self.data.load[width=simd_width](i * simd_width)\n",
        "            for j in range(simd_width):\n",
        "                if pred(vector[j]):\n",
        "                    return True\n",
        "        for k in range(remaining):\n",
        "            if pred(self.data.load[width=1](simd_blocks * simd_width + k)):\n",
        "                return True\n",
        "        return False\n",
        "\n",
        "    fn __eq__(self: Self, other: Self) raises -> Tensor[axes_sizes, DType.bool]:\n",
        "        if self.shape != other.shape:\n",
        "            raise Error(\"Dimension mismatch\")\n",
        "        copy = Tensor[axes_sizes, DType.bool](StaticTuple[Int, axes_sizes](0))\n",
        "        copy.shape = self.shape\n",
        "\n",
        "        @parameter\n",
        "        fn compare_elems[simd_width: Int](idx: Int):\n",
        "            copy.data.store[width=simd_width, volatile=True](\n",
        "                idx,\n",
        "                self.data.load[width=simd_width](idx)\n",
        "                == other.data.load[width=simd_width](idx),\n",
        "            )\n",
        "\n",
        "        vectorize[compare_elems, simdwidthof[DType.bool]()](copy.numels())\n",
        "        return copy\n",
        "\n",
        "    fn __add__(self: Self, other: Self) raises -> Tensor[axes_sizes, dtype]:\n",
        "        if self.shape != other.shape:\n",
        "            raise Error(\"Add -> Dimension mismatch\")\n",
        "        result = Tensor[axes_sizes, dtype](StaticTuple[Int, axes_sizes]())\n",
        "        result.shape = self.shape\n",
        "\n",
        "        @parameter\n",
        "        fn add_elems[simd_width: Int](idx: Int):\n",
        "            result.data.store[width=simd_width](\n",
        "                idx,\n",
        "                (\n",
        "                    self.data.load[width=simd_width](idx)\n",
        "                    + other.data.load[width=simd_width](idx)\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        vectorize[add_elems, simdwidthof[dtype]()](result.numels())\n",
        "        return result\n",
        "\n",
        "    fn __iadd__(self: Self, other: Self) raises:\n",
        "        if self.shape != other.shape:\n",
        "            raise Error(\"Add -> Dimension mismatch\")\n",
        "\n",
        "        @parameter\n",
        "        fn add_elems[simd_width: Int](idx: Int):\n",
        "            self.data.store[width=simd_width](\n",
        "                idx,\n",
        "                (\n",
        "                    self.data.load[width=simd_width](idx)\n",
        "                    + other.data.load[width=simd_width](idx)\n",
        "                ),\n",
        "            )\n",
        "\n",
        "        vectorize[add_elems, simdwidthof[dtype]()](self.numels())\n",
        "\n",
        "\n",
        "    fn exp(self) raises -> Tensor[axes_sizes, dtype]:\n",
        "        result = Tensor[axes_sizes, dtype](StaticTuple[Int, axes_sizes]())\n",
        "        result.shape = self.shape\n",
        "\n",
        "        @parameter\n",
        "        fn exp_elems[simd_width: Int](idx: Int):\n",
        "            result.data.store[width=simd_width](\n",
        "                idx, exp(self.data.load[width=simd_width](idx))\n",
        "            )\n",
        "\n",
        "        vectorize[exp_elems, simdwidthof[dtype]()](result.numels())\n",
        "        return result\n",
        "\n",
        "    fn __ne__(self: Self, other: Self) raises -> Tensor[axes_sizes, DType.bool]:\n",
        "        result = self == other\n",
        "\n",
        "        @parameter\n",
        "        fn invert[simd_width: Int](idx: Int):\n",
        "            result.data.store[width=simd_width](\n",
        "                idx, ~result.data.load[width=simd_width](idx)\n",
        "            )\n",
        "\n",
        "        vectorize[invert, simdwidthof[DType.bool]()](result.numels())\n",
        "        return result\n",
        "\n",
        "    fn zero_grad(self):\n",
        "        print(\"ok - zero grading coming\")\n",
        "        if self.requires_grad and self.grad:\n",
        "            print(\"ok - zero grading\")\n",
        "            memset_zero(self.grad[].data, self.numels())\n",
        "\n",
        "    @staticmethod\n",
        "    fn _init_grad_[axes_count: Int, datatype: DType, //](mut tensor: Tensor[axes_count, datatype], shape: StaticTuple[Int, axes_count]) raises:\n",
        "        if tensor.requires_grad and not tensor.grad:\n",
        "            # Gradients are float32\n",
        "            gradients = Tensor[axes_sizes=axes_count, dtype=DType.float32](\n",
        "                shape\n",
        "            )\n",
        "            ptr = Self.TensorAlias.alloc(1)\n",
        "            #print(\"ptr.type\", String(ptr.type))\n",
        "            print(\"ptr.address_space\", ptr.address_space.__str__())\n",
        "            print(\"ptr.alignment\", ptr.alignment.__str__())\n",
        "            print(\"ptr.mut\", ptr.mut.__str__())\n",
        "            #print(\"ptr.origin\", ptr.origin.__str__())\n",
        "            #tensor.grad = Self.TensorAlias.alloc(1)\n",
        "            #tensor.grad.init_pointee_move(gradients)\n",
        "            #tensor.zero_grad()\n",
        "            #Tensor.print(tensor.grad[])\n",
        "\n",
        "\n",
        "    fn __mul__(mut self: Self, factor: Scalar[dtype]) raises -> Self:\n",
        "        out = self\n",
        "\n",
        "        @parameter\n",
        "        fn mul_by_factor[simd_width: Int](idx: Int):\n",
        "            out.data.store[width=simd_width](\n",
        "                idx, out.data.load[width=simd_width](idx) * factor\n",
        "            )\n",
        "\n",
        "        vectorize[mul_by_factor, simdwidthof[dtype]()](out.numels())\n",
        "        fn _backward() raises -> None:\n",
        "            if self.requires_grad:\n",
        "                alias axes_count = axes_sizes\n",
        "                shape = self.shape.axes_sizes\n",
        "                Self._init_grad_(self, shape)\n",
        "                #Self._init_grad_[axes_count, DType.float32](self, shape)\n",
        "                out.requires_grad = True\n",
        "                #Self._init_grad_[axes_count, DType.float32](out, shape)\n",
        "                Self._init_grad_(out, shape)\n",
        "                print(\"in _backward\")\n",
        "                self.grad[] += out.grad[] * 2\n",
        "\n",
        "        out._backward = Optional(_backward)\n",
        "        return out\n",
        "\n",
        "    fn __add__(self: Self, value: Scalar[dtype]) -> Self:\n",
        "        copy = self\n",
        "\n",
        "        @parameter\n",
        "        fn add_value[simd_width: Int](idx: Int):\n",
        "            copy.data.store[width=simd_width](\n",
        "                idx, copy.data.load[width=simd_width](idx) + value\n",
        "            )\n",
        "\n",
        "        vectorize[add_value, simdwidthof[dtype]()](copy.numels())\n",
        "        return copy\n",
        "\n",
        "    fn __iadd__(self: Self, value: Scalar[dtype]):\n",
        "\n",
        "        @parameter\n",
        "        fn add_value[simd_width: Int](idx: Int):\n",
        "            self.data.store[width=simd_width](\n",
        "                idx, self.data.load[width=simd_width](idx) + value\n",
        "            )\n",
        "\n",
        "        vectorize[add_value, simdwidthof[dtype]()](self.numels())\n",
        "\n",
        "    fn __sub__(self: Self, value: Scalar[dtype]) -> Self:\n",
        "        copy = self\n",
        "\n",
        "        @parameter\n",
        "        fn subtract_value[simd_width: Int](idx: Int):\n",
        "            copy.data.store[width=simd_width](\n",
        "                idx, copy.data.load[width=simd_width](idx) - value\n",
        "            )\n",
        "\n",
        "        vectorize[subtract_value, simdwidthof[dtype]()](copy.numels())\n",
        "        return copy\n",
        "\n",
        "    fn __truediv__(self: Self, factor: Scalar[dtype]) -> Self:\n",
        "        copy = self\n",
        "\n",
        "        @parameter\n",
        "        fn div_by_factor[simd_width: Int](idx: Int):\n",
        "            copy.data.store[width=simd_width](\n",
        "                idx, copy.data.load[width=simd_width](idx).__truediv__(factor)\n",
        "            )\n",
        "\n",
        "        vectorize[div_by_factor, simdwidthof[dtype]()](copy.numels())\n",
        "        return copy\n",
        "\n",
        "    fn unsafe_ptr(self) -> UnsafePointer[Scalar[dtype]]:\n",
        "        return self.data\n",
        "\n",
        "    fn matmal(self, other: Self) raises -> Tensor[axes_sizes, dtype]:\n",
        "        start = perf_counter_ns()\n",
        "        from testing import assert_equal\n",
        "\n",
        "        result = Tensor[axes_sizes, dtype].zeros(self.shape[0], other.shape[1])\n",
        "        try:\n",
        "            assert_equal(self.shape[1], other.shape[0], \"matmul - Dim mismatch\")\n",
        "            for i in range(self.shape[0]):\n",
        "                for j in range(other.shape[1]):\n",
        "                    for k in range(self.shape[1]):\n",
        "                        result[i, j] += self[i, k] * other[k, j]\n",
        "        except e:\n",
        "            raise e\n",
        "        end = perf_counter_ns()\n",
        "        print(\"Total: \", end - start)\n",
        "        return result\n",
        "\n",
        "    fn load[\n",
        "        nelts: Int = 1\n",
        "    ](self, rows: Int, cols: Int) raises -> SIMD[dtype, nelts]:\n",
        "        from testing import assert_equal\n",
        "\n",
        "        try:\n",
        "            assert_equal(2, self.ndim(), \"load is supported only for 2d tensor\")\n",
        "        except e:\n",
        "            raise e\n",
        "        return self.data.load[width=nelts](rows * self.shape[1] + cols)\n",
        "\n",
        "    fn store[\n",
        "        nelts: Int = 1\n",
        "    ](self, rows: Int, cols: Int, val: SIMD[dtype, nelts]) raises:\n",
        "        from testing import assert_equal\n",
        "\n",
        "        try:\n",
        "            assert_equal(\n",
        "                2, self.ndim(), \"store is supported only for 2d tensor\"\n",
        "            )\n",
        "        except e:\n",
        "            raise e\n",
        "        self.data.store(rows * self.shape[1] + cols, val)\n",
        "\n",
        "    fn matmal_v2(self, other: Self) raises -> Tensor[axes_sizes, dtype]:\n",
        "        start = perf_counter_ns()\n",
        "        from testing import assert_equal\n",
        "\n",
        "        result = Tensor[axes_sizes, dtype].zeros(self.shape[0], other.shape[1])\n",
        "        try:\n",
        "            assert_equal(self.shape[1], other.shape[0], \"matmul - Dim mismatch\")\n",
        "            for i in range(self.shape[0]):\n",
        "                for j in range(self.shape[1]):\n",
        "                    for k in range(other.shape[1]):\n",
        "                        result[i, k] += self[i, j] * other[j, k]\n",
        "        except e:\n",
        "            raise e\n",
        "        end = perf_counter_ns()\n",
        "        print(\"Total: \", end - start)\n",
        "\n",
        "        return result\n",
        "\n",
        "    fn matmal_v3(self, other: Self) raises -> Tensor[axes_sizes, dtype]:\n",
        "        start = perf_counter_ns()\n",
        "        from testing import assert_equal\n",
        "\n",
        "        result = Tensor[axes_sizes, dtype].zeros(self.shape[0], other.shape[1])\n",
        "        try:\n",
        "            assert_equal(self.shape[1], other.shape[0], \"matmul - Dim mismatch\")\n",
        "            for i in range(self.shape[0]):\n",
        "                for j in range(self.shape[1]):\n",
        "\n",
        "                    @parameter\n",
        "                    fn dot[simd_width: Int](idx: Int):\n",
        "                        try:\n",
        "                            result.store[simd_width](\n",
        "                                i,\n",
        "                                idx,\n",
        "                                result.load[simd_width](i, idx)\n",
        "                                + self[i, j] * other.load[simd_width](j, idx),\n",
        "                            )\n",
        "                        except e:\n",
        "                            print(e)\n",
        "\n",
        "                    vectorize[dot, 2 * simdwidthof[dtype]()](other.shape[1])\n",
        "        except e:\n",
        "            raise e\n",
        "        end = perf_counter_ns()\n",
        "        print(\"Total: \", end - start)\n",
        "\n",
        "        return result\n",
        "\n",
        "    fn reshape[\n",
        "        new_axes_sizes: Int\n",
        "    ](self, *newdims: Int) raises -> Tensor[new_axes_sizes, dtype]:\n",
        "        shape = Self.init_shape[new_axes_sizes](newdims)  # StaticTuple\n",
        "        numels = 1\n",
        "        for idx in range(len(shape)):\n",
        "            numels *= shape[idx]\n",
        "        if numels != self.numels():\n",
        "            raise Error(\n",
        "                \"Tensor with \"\n",
        "                + String(self.numels())\n",
        "                + \" elements can't be converted to \"\n",
        "                + int_varia_list_to_str(newdims)\n",
        "                + \" dimensional tensor\"\n",
        "            )\n",
        "        result = Tensor[new_axes_sizes, dtype](shape)\n",
        "\n",
        "        @parameter\n",
        "        fn copy_elements[simd_width: Int](idx: Int):\n",
        "            result.data.store[width=simd_width](\n",
        "                idx, self.data.load[width=simd_width](idx)\n",
        "            )\n",
        "\n",
        "        vectorize[copy_elements, simdwidthof[dtype]()](self.numels())\n",
        "        return result\n",
        "\n",
        "    fn __str__(self) -> String:\n",
        "        s = String(\"[\")\n",
        "        if axes_sizes == 1:\n",
        "            s += \"1D Tensor\"\n",
        "        elif axes_sizes == 2:\n",
        "            s += \"2D Tensor\"\n",
        "        elif axes_sizes == 3:\n",
        "            s += \"3D Tensor\"\n",
        "        elif axes_sizes == 4:\n",
        "            s += \"4D Tensor\"\n",
        "        else:\n",
        "            s += \"Unsupported Tensor\"\n",
        "        s += self.shape.__str__()\n",
        "        s += \", Type: \" + self.datatype.__str__()\n",
        "        s += \", requires_grad: \" + String(self.requires_grad)\n",
        "        s += \"]\"\n",
        "        return s\n",
        "\n",
        "    @staticmethod\n",
        "    fn rand(\n",
        "        *tensor_shapes: Int,\n",
        "        init_seed: Optional[Int] = None,\n",
        "        requires_grad: Bool = False,\n",
        "    ) raises -> Tensor[axes_sizes, dtype]:\n",
        "        if init_seed:\n",
        "            seed(init_seed.value())\n",
        "        else:\n",
        "            seed()\n",
        "        axes_dims = Self.init_shape[axes_sizes](tensor_shapes)\n",
        "        tensor = Tensor[axes_sizes, dtype](axes_dims, requires_grad)\n",
        "        randn(tensor.data, tensor.numels())\n",
        "        return tensor\n",
        "\n",
        "    @staticmethod\n",
        "    fn arange[\n",
        "        d_type: DType = DType.float32\n",
        "    ](end: Int, start: Int = 0) raises -> Tensor[1, d_type]:\n",
        "        len = end - start\n",
        "        result = Tensor[dtype=d_type](len)\n",
        "        iota(result.data, len, offset=start)\n",
        "        return result\n",
        "\n",
        "    @staticmethod\n",
        "    fn zeros(*tensor_shapes: Int) raises -> Tensor[axes_sizes, dtype]:\n",
        "        axes_dims = Self.init_shape[axes_sizes](tensor_shapes)\n",
        "        tensor = Tensor[axes_sizes, dtype](axes_dims)\n",
        "        memset_zero(tensor.data, tensor.numels())\n",
        "        return tensor\n",
        "\n",
        "    @staticmethod\n",
        "    fn ones(*tensor_shapes: Int) raises -> Tensor[axes_sizes, dtype]:\n",
        "        axes_dims = Self.init_shape[axes_sizes](tensor_shapes)\n",
        "        tensor = Tensor[axes_sizes, dtype](axes_dims)\n",
        "        var value: SIMD[dtype, 1]\n",
        "\n",
        "        @parameter\n",
        "        if dtype.is_floating_point():\n",
        "            value = SIMD[dtype, 1](1.0)\n",
        "        else:\n",
        "            value = SIMD[dtype, 1](1)\n",
        "        for i in range(tensor.numels()):\n",
        "            tensor.data.store(i, value)\n",
        "        return tensor\n",
        "\n",
        "    fn print_tensor_recursive(self, mut indices: List[Int], level: Int) raises:\n",
        "        try:\n",
        "            current_dim = len(indices)\n",
        "            indent = \" \" * (level * 2)\n",
        "\n",
        "            num_first = 5  # Show first 5 elements\n",
        "            num_last = 5  # Show last 5 elements\n",
        "\n",
        "            if current_dim == self.ndim() - 1:\n",
        "                print(indent + \"[\", end=\"\")\n",
        "                size = self.shape[current_dim]\n",
        "\n",
        "                for i in range(size):\n",
        "                    if i < num_first:\n",
        "                        indices.append(i)\n",
        "                        print(\n",
        "                            self[indices],\n",
        "                            end=\", \" if (\n",
        "                                i != num_first - 1\n",
        "                                or size > num_first + num_last\n",
        "                            ) else \"\",\n",
        "                        )\n",
        "                        _ = indices.pop()\n",
        "                    elif i == num_first:\n",
        "                        if size > num_first + num_last:\n",
        "                            print(\"..., \", end=\"\")\n",
        "                        # Skip printing middle elements\n",
        "                    elif i >= size - num_last:\n",
        "                        indices.append(i)\n",
        "                        print(self[indices], end=\", \" if i != size - 1 else \"\")\n",
        "                        _ = indices.pop()\n",
        "                print(\"]\", end=\"\")\n",
        "            else:\n",
        "                print(indent + \"[\")\n",
        "                size = self.shape[current_dim]\n",
        "\n",
        "                for i in range(size):\n",
        "                    if i < num_first:\n",
        "                        indices.append(i)\n",
        "                        self.print_tensor_recursive(indices, level + 1)\n",
        "                        _ = indices.pop()\n",
        "                        if i != num_first - 1 or size > num_first + num_last:\n",
        "                            print(\",\")\n",
        "                    elif i == num_first:\n",
        "                        if size > num_first + num_last:\n",
        "                            print(indent + \"  ...,\")\n",
        "                    elif i >= size - num_last:\n",
        "                        indices.append(i)\n",
        "                        self.print_tensor_recursive(indices, level + 1)\n",
        "                        _ = indices.pop()\n",
        "                        if i != size - 1:\n",
        "                            print(\",\")\n",
        "                print(indent + \"]\", end=\"\")\n",
        "                print(\"\\n\")\n",
        "        except e:\n",
        "            print(e)\n",
        "\n",
        "    @staticmethod\n",
        "    fn print(t: Tensor):\n",
        "        print(t.__str__())\n",
        "        print()\n",
        "        l = List[Int]()\n",
        "        try:\n",
        "            t.print_tensor_recursive(l, 1)\n",
        "        except e:\n",
        "            print(e)\n",
        "\n",
        "    fn __iter__(\n",
        "        ref self,\n",
        "    ) -> TensorElemIter[axes_sizes, dtype, __origin_of(self)]:\n",
        "        \"\"\"Iterate over elements of the tensor, returning mutable references.\n",
        "\n",
        "        Returns:\n",
        "            An iterator of mutable references to the tensor elements.\n",
        "        \"\"\"\n",
        "        return TensorElemIter(0, Pointer(to=self))\n",
        "\n",
        "\n",
        "def main():\n",
        "    # tensor = Tensor[5].rand(4, 3, 2, 1)\n",
        "    # Tensor.print(Tensor.arange(7, start=3).reshape[2](2, 2))\n",
        "    tensor = Tensor[2].rand(4, 3, requires_grad=True)\n",
        "    Tensor.print(tensor)\n",
        "    multiplied = tensor * 2\n",
        "    Tensor.print(multiplied)\n",
        "    if multiplied._backward:\n",
        "        multiplied._backward.value()()\n",
        "        #multiplied._backward.value()()\n",
        "    #tensor._init_grad_()\n",
        "    _ = \"\"\"print(\"Am I gone: \")\n",
        "    Tensor.print(tensor)\n",
        "    print()\n",
        "    multiplied = tensor / 2\n",
        "    print(\"I am multiplied: \")\n",
        "    Tensor.print(multiplied)\n",
        "    print()\n",
        "\n",
        "    rival = tensor == multiplied\n",
        "    print(\"rival\")\n",
        "    Tensor.print(rival)\n",
        "\n",
        "    tensor = Tensor[2].rand(4, 3)\n",
        "    print(\"Original\")\n",
        "    Tensor.print(tensor)\n",
        "    reshaped = tensor.reshape[3](2, 2, 3)\n",
        "    print(\"Reshaped\")\n",
        "    Tensor.print(reshaped)\n",
        "\n",
        "    tensor_false = Tensor[2, DType.bool].zeros(4, 3)\n",
        "    indices = List[Int]()\n",
        "    tensor_false.print_tensor_recursive(indices, 1)\n",
        "\n",
        "    tensor_true = Tensor[2, DType.bool].ones(4, 3)\n",
        "    indices = List[Int]()\n",
        "    tensor_true.print_tensor_recursive(indices, 1)\n",
        "\n",
        "    tensor = Tensor[2].ones(4, 3)\n",
        "    indices = List[Int]()\n",
        "    tensor.print_tensor_recursive(indices, 1)\n",
        "\n",
        "    t16 = Tensor[2, DType.uint16].zeros(5, 5)\n",
        "    t16[0, 0] = 1\n",
        "    t16[0, 1] = 2\n",
        "    t16[0, 2] = 3\n",
        "    t16[0, 3] = 4\n",
        "    t16[0, 4] = 5\n",
        "\n",
        "    t16[1, 0] = 6\n",
        "    t16[1, 1] = 7\n",
        "    t16[1, 2] = 8\n",
        "    t16[1, 3] = 9\n",
        "    t16[1, 4] = 10\n",
        "\n",
        "    t16[2, 0] = 11\n",
        "    t16[2, 1] = 12\n",
        "    t16[2, 2] = 13\n",
        "    t16[2, 3] = 14\n",
        "    t16[2, 4] = 15\n",
        "\n",
        "    t16[3, 0] = 16\n",
        "    t16[3, 1] = 17\n",
        "    t16[3, 2] = 18\n",
        "    t16[3, 3] = 19\n",
        "    t16[3, 4] = 20\n",
        "\n",
        "    t16[4, 0] = 21\n",
        "    t16[4, 1] = 22\n",
        "    t16[4, 2] = 23\n",
        "    t16[4, 3] = 24\n",
        "    t16[4, 4] = 25\n",
        "\n",
        "    other = Tensor[2, DType.uint16].zeros(5, 5)\n",
        "    other[0, 0] = 10\n",
        "    other[0, 1] = 2\n",
        "    other[0, 2] = 3\n",
        "    other[0, 3] = 4\n",
        "    other[0, 4] = 7\n",
        "\n",
        "    other[1, 0] = 6\n",
        "    other[1, 1] = 7\n",
        "    other[1, 2] = 8\n",
        "    other[1, 3] = 9\n",
        "    other[1, 4] = 10\n",
        "\n",
        "    other[2, 0] = 13\n",
        "    other[2, 1] = 14\n",
        "    other[2, 2] = 15\n",
        "    other[2, 3] = 16\n",
        "    other[2, 4] = 17\n",
        "\n",
        "    other[3, 0] = 18\n",
        "    other[3, 1] = 19\n",
        "    other[3, 2] = 20\n",
        "    other[3, 3] = 21\n",
        "    other[3, 4] = 22\n",
        "\n",
        "    other[4, 0] = 23\n",
        "    other[4, 1] = 24\n",
        "    other[4, 2] = 25\n",
        "    other[4, 3] = 26\n",
        "    other[4, 4] = 25\n",
        "\n",
        "    # Tensor.print(t16.matmal_v2(other))\n",
        "    print()\n",
        "\n",
        "    # Tensor.print(t16.matmal_v3(other))\n",
        "    print()\n",
        "\n",
        "    # Tensor.print(t16.matmal(other))\n",
        "    Tensor.print(t16 == other)\n",
        "    Tensor.print(t16 != other)\n",
        "\n",
        "    tensor_big1 = Tensor[2].rand(1024, 4096)\n",
        "    tensor_big2 = Tensor[2].rand(4096, 512)\n",
        "\n",
        "    # Tensor.print(tensor_big1.matmal_v3(tensor_big2))\n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "struct Shape[axes: Int]:\n",
        "    var axes_sizes: StaticTuple[Int, axes]\n",
        "    var ndim: Int\n",
        "    var numels: Int\n",
        "\n",
        "    fn __ne__(self, other: Self) -> Bool:\n",
        "        return self.__eq__(other) == False\n",
        "\n",
        "    fn __eq__(self, other: Self) -> Bool:\n",
        "        length1 = len(self.axes_sizes)\n",
        "        length2 = len(other.axes_sizes)\n",
        "        if length1 != length2:\n",
        "            return False\n",
        "        for i in range(length1):\n",
        "            if self.axes_sizes[i] != other.axes_sizes[i]:\n",
        "                return False\n",
        "        return self.ndim == other.ndim and self.numels == other.numels\n",
        "\n",
        "    fn __init__(out self, array: StaticTuple[Int, axes]):\n",
        "        self.axes_sizes = array\n",
        "        self.ndim = axes\n",
        "        if len(array) > 0:\n",
        "            self.numels = 1\n",
        "            for i in range(axes):\n",
        "                self.numels *= self.axes_sizes[i]\n",
        "        else:\n",
        "            self.numels = 0\n",
        "\n",
        "    fn flatten_index(self, indices: StaticTuple[Int, size=axes]) -> Int:\n",
        "        index = 0\n",
        "        stride = 1\n",
        "        for i in reversed(range(self.ndim)):\n",
        "            idx = indices[i]\n",
        "            self_idx = self[i]\n",
        "            if idx >= self_idx:\n",
        "                return -1\n",
        "            index += idx * stride\n",
        "            stride *= self_idx\n",
        "        return index\n",
        "\n",
        "    fn __str__(self) -> String:\n",
        "        s = String(\"(\")\n",
        "        for i in range(axes):\n",
        "            s += String(self.axes_sizes[i])\n",
        "            if i < axes - 1:\n",
        "                s += \", \"\n",
        "        s += \")\"\n",
        "        return s\n",
        "\n",
        "    fn __repr__(self) -> String:\n",
        "        return self.__str__()\n",
        "\n",
        "    fn write_to[W: Writer](self, mut writer: W):\n",
        "        s = self.__str__()\n",
        "        writer.write(s)\n",
        "\n",
        "    fn __getitem__(self, index: Int) -> Int:\n",
        "        if 0 <= index < axes:\n",
        "            return self.axes_sizes[index]\n",
        "        else:\n",
        "            return -1\n",
        "\n",
        "    fn __moveinit__(out self, owned other: Self):\n",
        "        self.axes_sizes = other.axes_sizes\n",
        "        self.ndim = other.ndim\n",
        "        self.numels = other.numels\n",
        "\n",
        "    fn __copyinit__(out self, other: Self):\n",
        "        self.axes_sizes = other.axes_sizes\n",
        "        self.ndim = other.ndim\n",
        "        self.numels = other.numels\n",
        "\n",
        "\n",
        "@value\n",
        "struct TensorElemIter[\n",
        "    mutability: Bool, //,\n",
        "    axes_count: Int,\n",
        "    dtype: DType,\n",
        "    origin: Origin[mutability],\n",
        "    forward: Bool = True,\n",
        "]:\n",
        "    \"\"\"Iterator for Tensor element.\n",
        "\n",
        "    Parameters:\n",
        "        mutability: Whether the reference to the Tensor is mutable.\n",
        "        axes_count: Tensor dimension.\n",
        "        dtype: The type of the elements in the tensor.\n",
        "        origin: The origin of the Tensor.\n",
        "        forward: The iteration direction. `False` is backwards.\n",
        "    \"\"\"\n",
        "\n",
        "    alias tensor = Tensor[axes_count, dtype]\n",
        "\n",
        "    var index: Int\n",
        "    var src: Pointer[Self.tensor, origin]\n",
        "\n",
        "    fn __iter__(self) -> Self:\n",
        "        return self\n",
        "\n",
        "    fn __next__(\n",
        "        mut self, out p: Pointer[Scalar[dtype], MutableAnyOrigin]\n",
        "    ) raises:\n",
        "        @parameter\n",
        "        if forward:\n",
        "            p = Pointer(to=self.src[].data[self.index])\n",
        "            self.index += 1\n",
        "        else:\n",
        "            self.index -= 1\n",
        "            p = Pointer(to=self.src[].data[self.index])\n",
        "\n",
        "    @always_inline\n",
        "    fn __has_next__(self) -> Bool:\n",
        "        return self.__len__() > 0\n",
        "\n",
        "    fn __len__(self) -> Int:\n",
        "        @parameter\n",
        "        if forward:\n",
        "            return len(self.src[]) - self.index\n",
        "        else:\n",
        "            return self.index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2k9wkDaONiz"
      },
      "outputs": [],
      "source": [
        "!magic run mojo tensors.mojo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bSglX7bNONi0"
      },
      "outputs": [],
      "source": [
        "!magic run mojo format tensors.mojo"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat tensors.mojo"
      ],
      "metadata": {
        "id": "z2OxrAfbJYKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!magic run mojo format common_utils.mojo"
      ],
      "metadata": {
        "id": "pOYK25H5YsvT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}